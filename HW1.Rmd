---
title: "DS HW1"
author: "Ruihan Ding"
date: "2026-02-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
library(ggrepel)
library(tidyverse)
library(pls)
```

Import the data.

```{r}
housing_train = 
  read_csv("housing_training.csv") |> 
  janitor::clean_names()

housing_test = 
  read_csv("housing_test.csv") |> 
  janitor::clean_names()
```

# a

* Fit a lasso model on the training data.

```{r}
ctrl1 = trainControl(method = "cv", 
                     number = 10,
                     selectionFunction = "best")

set.seed(2026)
lasso.fit = train(sale_price ~ .,
                  data = housing_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1, 
                                         lambda = exp(seq(8, 2, length = 100))),
                  trControl = ctrl1)

plot(lasso.fit, xTrans = log)
```

* Report the selected tuning parameter and the test error.

```{r}
lasso_lambda_min = lasso.fit$bestTune$lambda

lasso.pred = predict(lasso.fit, newdata = housing_test)
lasso_test_mse = mean((lasso.pred - housing_test[["sale_price"]])^2)
```

_The selected lambda is `r lasso_lambda_min`, and the test error is `r lasso_test_mse`._

* When the 1SE rule is applied, how many predictors are included in the model?

```{r}
ctrl_1se = trainControl(method = "cv", 
                     number = 10,
                     selectionFunction = "oneSE")

set.seed(2026)
lasso_1se.fit = train(sale_price ~ .,
                      data = housing_train,
                      method = "glmnet",
                      tuneGrid = expand.grid(alpha = 1,
                                             lambda = exp(seq(8, 2, length = 100))),
                      trControl = ctrl_1se)

lasso_1se = lasso_1se.fit$bestTune$lambda
lasso_1se_coef = coef(lasso_1se.fit$finalModel, lasso_1se)
lasso_1se_coef
```

_Under the 1SE rule, the model includes `r sum(lasso_1se_coef != 0) - 1` predictors._

# b

* Fit an elastic net model on the training data.

```{r}
set.seed(2026)
enet.fit = train(sale_price ~ .,
                 data = housing_train,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                        lambda = exp(seq(10, 2, length = 100))),
                 trControl = ctrl1)

myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)
```

* Report the selected tuning parameters and the test error.

```{r}
enet.fit$bestTune
enet_alpha = enet.fit$bestTune$alpha
enet_lambda_min = enet.fit$bestTune$lambda

enet.pred = predict(enet.fit, newdata = housing_test)
enet_test_mse = mean((enet.pred - housing_test[["sale_price"]])^2)
```

_The selected alpha is `r enet_alpha`, lambda is `r enet_lambda_min`, and the test error is `r enet_test_mse`._

* Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why.

```{r}
set.seed(2026)
enet_1se.fit = train(sale_price ~ .,
                     data = housing_train,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = enet_alpha,
                                            lambda = exp(seq(10, 2, length = 100))),
                     trControl = ctrl_1se)

enet_1se = enet_1se.fit$bestTune$lambda
```

_Based on the previously selected optimal alpha = `r enet_alpha`, I applied the 1SE rule to determine lambda, which is `r enet_1se`._

# c

* Fit a partial least squares model on the training data and report the test error. 

```{r}
x = model.matrix(sale_price ~ ., housing_train)[, -1]
y = housing_train[["sale_price"]]

x2 = model.matrix(sale_price ~ ., housing_test)[, -1]
y2 = housing_test[["sale_price"]]

set.seed(2026)
pls_fit = train(x, y,
                method = "pls",
                tuneGrid = data.frame(ncomp = 1:19),
                trControl = ctrl1,
                preProcess = c("center", "scale"))

predy_pls = predict(pls_fit, newdata = x2)
pls_test_mse = mean((y2 - predy_pls)^2)
```

_The test error is `r pls_test_mse`._

* How many components are included in your model?

```{r}
ggplot(pls_fit, highlight = TRUE)
```

_There are `r pls_fit$bestTune$ncomp` components in my model._

# d

* Choose the best model for predicting the response and explain your choice.

```{r}
resamp = resamples(list(lasso = lasso.fit, lasso_1se = lasso_1se.fit, enet = enet.fit, enet_1se = enet_1se.fit, pls = pls_fit))

summary(resamp)
```

_Partial least squares model is the best choice for predicting the response, because it achieves the lowest cross-validated RMSE._

# e

* If R package “caret” was used for the lasso in (a), retrain this model using R package “glmnet”, and vice versa.

```{r}
set.seed(2026)
cv.lasso = cv.glmnet(x, y, 
                     alpha = 1,
                     lambda = exp(seq(8, 2, length = 100)))
```

* Compare the selected tuning parameters between the two software approaches. Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.

```{r}
plot(cv.lasso)
cv.lasso$lambda.min
cv.lasso$lambda.1se
```

_The selected lambda.min for caret is `r lasso_lambda_min`, while for glmnet is `r cv.lasso$lambda.min`. The lambda.1se for caret is `r lasso_1se`, while for glmnet is `r cv.lasso$lambda.1se`._

_These discrepancies are expected because caret and glmnet implement cross-validation differently. Although the same lambda grid was used, the two functions generate folds independently and compute cross-validated errors and their variability using different internal procedures. Since both lambda.min and lambda.1se depend on the estimated cross-validation error curve and its standard error, even small differences in fold assignment and error aggregation can lead to different selected tuning parameters._









